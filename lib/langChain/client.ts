// lib/langChain/client.ts - Optimized OpenRouter + Grok-4 Configuration with Multi-Key Support + Caching
import { ChatOpenAI } from "@langchain/openai";
import { apiKeyManager } from "./api-key-manager";

// Simple in-memory cache for LLM responses
class SimpleCache {
  private cache: Map<string, { response: any; timestamp: number }> = new Map();
  private ttl: number = 3600000; // 1 hour TTL

  get(key: string): any | null {
    const item = this.cache.get(key);
    if (!item) return null;
    
    // Check if expired
    if (Date.now() - item.timestamp > this.ttl) {
      this.cache.delete(key);
      return null;
    }
    
    console.log(`ğŸ’¾ Cache HIT: ${key.substring(0, 50)}...`);
    return item.response;
  }

  set(key: string, response: any): void {
    this.cache.set(key, {
      response,
      timestamp: Date.now()
    });
    console.log(`ğŸ’¾ Cache SET: ${key.substring(0, 50)}...`);
  }

  clear(): void {
    this.cache.clear();
    console.log('ğŸ§¹ Cache cleared');
  }

  size(): number {
    return this.cache.size;
  }
}

// Global cache instance
let globalCache: SimpleCache | null = null;

function getCache(): SimpleCache {
  if (!globalCache) {
    globalCache = new SimpleCache();
    console.log('ğŸ’¾ LLM Cache initialized');
  }
  return globalCache;
}

/**
 * Clear the global cache
 */
export function clearLLMCache() {
  getCache().clear();
}

/**
 * Get cache statistics
 */
export function getCacheStats() {
  const cache = getCache();
  return {
    size: cache.size(),
    enabled: true
  };
}

// Grok-4 æ¨¡å‹é…ç½®é€‰é¡¹
export interface GrokConfig {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  enableReasoning?: boolean;
  streaming?: boolean;
  timeout?: number;
  keySelectionStrategy?: 'round_robin' | 'least_used' | 'best_performance';
  maxRetries?: number;
  enableCache?: boolean; // æ˜¯å¦å¯ç”¨ç¼“å­˜
}

// é»˜è®¤é…ç½®
const DEFAULT_GROK_CONFIG: GrokConfig = {
  model: process.env.OPEN_ROUTER_MODEL || "z-ai/glm-4.5-air:free",
  temperature: 0.3,
  maxTokens: 8000, // Reduced to fit within 16K context limit (leaving room for input tokens)
  topP: 1.0,
  frequencyPenalty: 0,
  presencePenalty: 0,
  enableReasoning: false, // é»˜è®¤å…³é—­æ¨ç†æ¨¡å¼ä»¥èŠ‚çœæˆæœ¬
  streaming: true, // å¯ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
  timeout: 60000, // 60ç§’è¶…æ—¶
  keySelectionStrategy: 'round_robin', // é»˜è®¤è½®è¯¢ç­–ç•¥
  maxRetries: 3, // é»˜è®¤é‡è¯•3æ¬¡
  enableCache: true, // é»˜è®¤å¯ç”¨ç¼“å­˜
};

// è·å–ç«™ç‚¹ä¿¡æ¯ç”¨äºOpenRouteræ’å
const getSiteInfo = () => {
  const isDevelopment = process.env.NODE_ENV === 'development';
  const defaultUrl = isDevelopment 
    ? "http://localhost:3000" 
    : "https://studify-platform.vercel.app";
    
  // åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œä¼˜å…ˆä½¿ç”¨ localhost
  const siteUrl = isDevelopment 
    ? "http://localhost:3000"
    : (process.env.NEXT_PUBLIC_SITE_URL || defaultUrl);
    
  return {
    siteUrl,
    siteName: process.env.NEXT_PUBLIC_SITE_NAME || "Studify",
  };
};

/**
 * åˆ›å»ºä¼˜åŒ–çš„Grok-4 LLMå®ä¾‹ - æ”¯æŒå¤škeyè½®æ¢
 * @param config è‡ªå®šä¹‰é…ç½®é€‰é¡¹
 * @returns ChatOpenAIå®ä¾‹
 */
export async function getLLM(config: Partial<GrokConfig> = {}) {
  // Set dummy OPENAI_API_KEY to prevent LangChain internal checks
  // This is safe because we override the baseURL to OpenRouter


  const finalConfig = { ...DEFAULT_GROK_CONFIG, ...config };
  const { siteUrl, siteName } = getSiteInfo();

  // ä»key managerè·å–å¯ç”¨çš„API key
  const { key: apiKey, name: keyName } = await apiKeyManager.getAvailableKey(
    finalConfig.keySelectionStrategy
  );

  console.log(`ğŸ”‘ Using OpenRouter key: ${keyName} (${apiKey.substring(0, 12)}...${apiKey.substring(apiKey.length - 4)})`);

  // æ„é€ æ¨¡å‹å‚æ•°
  const modelParams: any = {
    model: finalConfig.model,
    temperature: finalConfig.temperature,
    maxTokens: finalConfig.maxTokens,
    topP: finalConfig.topP,
    frequencyPenalty: finalConfig.frequencyPenalty,
    presencePenalty: finalConfig.presencePenalty,
    streaming: finalConfig.streaming,
    timeout: finalConfig.timeout,
    maxRetries: finalConfig.maxRetries,
  };

  // Grok-4æ¨ç†æ¨¡å¼é…ç½®
  if (finalConfig.enableReasoning) {
    modelParams.reasoning = { enabled: true };
  }

  const chatOpenAI = new ChatOpenAI({
    ...modelParams,
    openAIApiKey: apiKey, // LangChain still needs this parameter
    configuration: {
      baseURL: "https://openrouter.ai/api/v1",
      defaultHeaders: {
        "Authorization": `Bearer ${apiKey}`, // OpenRouter éœ€è¦è¿™ä¸ªå¤´
        "HTTP-Referer": siteUrl, // ç”¨äºOpenRouteræ’åç»Ÿè®¡
        "X-Title": siteName, // ç”¨äºOpenRouteræ’åæ˜¾ç¤º
      },
    },
    callbacks: [{
      handleLLMEnd: async () => {
        await apiKeyManager.recordUsage(keyName, true);
      },
      handleLLMError: async (error) => {
        await apiKeyManager.recordUsage(keyName, false, error);
      }
    }]
  });

  // æ·»åŠ keyä¿¡æ¯åˆ°å®ä¾‹
  (chatOpenAI as any).__keyName = keyName;
  return chatOpenAI;
}

/**
 * è·å–æ¨ç†æ¨¡å¼çš„DeepSeekå®ä¾‹ï¼ˆæ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä½†æˆæœ¬æ›´é«˜ï¼‰
 */
export function getReasoningLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    enableReasoning: true,
    temperature: 0.1, // æ¨ç†æ¨¡å¼ä½¿ç”¨æ›´ä½çš„æ¸©åº¦
    model: process.env.OPEN_ROUTER_REASONING_MODEL || process.env.OPEN_ROUTER_MODEL || "z-ai/glm-4.5-air:free",
  });
}

/**
 * è·å–åˆ›æ„å†™ä½œä¸“ç”¨çš„DeepSeekå®ä¾‹
 */
export function getCreativeLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: process.env.OPEN_ROUTER_CREATIVE_MODEL || process.env.OPEN_ROUTER_MODEL || "z-ai/glm-4.5-air:free",
    temperature: 0.8, // é«˜æ¸©åº¦å¢åŠ åˆ›æ„æ€§
    topP: 0.9,
    frequencyPenalty: 0.1,
    presencePenalty: 0.1,
  });
}

/**
 * è·å–åˆ†æä¸“ç”¨çš„DeepSeekå®ä¾‹
 */
export function getAnalyticalLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: process.env.OPEN_ROUTER_ANALYTICAL_MODEL || process.env.OPEN_ROUTER_MODEL || "z-ai/glm-4.5-air:free",
    temperature: 0.1, // ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
    topP: 0.95,
    enableReasoning: false, // DeepSeek doesn't support reasoning mode
  });
}

/**
 * è·å–å¤§ä¸Šä¸‹æ–‡çª—å£çš„DeepSeekå®ä¾‹ï¼ˆç”¨äºé•¿æ–‡æ¡£å¤„ç†ï¼‰
 */
export function getLongContextLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: process.env.OPEN_ROUTER_LONG_CONTEXT_MODEL || process.env.OPEN_ROUTER_MODEL || "z-ai/glm-4.5-air:free",
    maxTokens: 32768, // ä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£
    temperature: 0.2,
  });
}

/**
 * è·å–è§†è§‰æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºå›¾æ–‡ç†è§£ï¼‰- ä½¿ç”¨Kimi VLæ¨¡å‹
 */
export function getVisionLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: process.env.OPEN_ROUTER_IMAGE_MODEL || "moonshotai/kimi-vl-a3b-thinking:free", // Use image model for vision tasks
    temperature: 0.3,
    maxTokens: 4096,
  });
}

// å¯¼å‡ºé…ç½®å¸¸é‡ä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
export { DEFAULT_GROK_CONFIG };
