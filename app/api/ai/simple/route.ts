import { NextRequest, NextResponse } from 'next/server';
import { aiWorkflowExecutor } from '@/lib/langChain/ai-workflow';
import { authorize } from '@/utils/auth/server-guard';
import { createRateLimitCheck, rateLimitResponse } from '@/lib/ratelimit';

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  
  try {
    // éªŒè¯ç”¨æˆ·èº«ä»½
    const authResult = await authorize('student');
    if (authResult instanceof NextResponse) {
      return authResult;
    }

    const { user, payload } = authResult;
    const profile = user.profile;
    const userId = profile?.id || parseInt(payload.sub);
    
    // âœ… Rate limiting check
    const checkLimit = createRateLimitCheck('ai');
    const { allowed, remaining, resetTime, limit } = checkLimit(userId.toString());
    
    if (!allowed) {
      return NextResponse.json(
        rateLimitResponse(resetTime, limit),
        { 
          status: 429,
          headers: {
            'X-RateLimit-Limit': limit.toString(),
            'X-RateLimit-Remaining': '0',
            'X-RateLimit-Reset': resetTime.toString()
          }
        }
      );
    }

    const body = await request.json();
    
    const { 
      prompt, 
      model, 
      temperature,
      includeContext,
      contextQuery,
      contextConfig
    } = body;

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!prompt) {
      return NextResponse.json({
        error: 'Missing required parameter: prompt'
      }, { status: 400 });
    }

    console.log(`ğŸ¤– Simple AI call for user ${userId}`);

    // âœ… æ‰§è¡Œç®€å•AIè°ƒç”¨
    // Note: Caching is handled internally by getLLM() with enableCache: true
    const result = await aiWorkflowExecutor.simpleAICall(prompt, {
      model,
      temperature,
      userId,
      includeContext,
      contextQuery: contextQuery || prompt, // é»˜è®¤ä½¿ç”¨promptä½œä¸ºcontextæŸ¥è¯¢
      contextConfig
    });

    const processingTime = Date.now() - startTime;
    console.log(`âœ… Simple AI call completed in ${processingTime}ms`);

    return NextResponse.json({
      result,
      success: true,
      metadata: {
        model: model || 'default',
        includeContext: includeContext || false,
        cached: false, // TODO: ä» cache ç³»ç»Ÿè·å–
        processingTimeMs: processingTime,
        timestamp: new Date().toISOString()
      }
    }, {
      headers: {
        'X-RateLimit-Limit': limit.toString(),
        'X-RateLimit-Remaining': remaining.toString(),
        'X-RateLimit-Reset': resetTime.toString()
      }
    });

  } catch (error) {
    console.error('âŒ Simple AI API error:', error);
    
    const processingTime = Date.now() - startTime;
    
    return NextResponse.json({
      error: 'AI processing failed',
      details: error instanceof Error ? error.message : 'Unknown error',
      success: false,
      metadata: {
        processingTimeMs: processingTime,
        timestamp: new Date().toISOString()
      }
    }, { status: 500 });
  }
}
