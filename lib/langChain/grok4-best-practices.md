# ğŸš€ Grok-4 + LangChain æœ€ä½³å®è·µæŒ‡å—

## ğŸ“‹ ç›®å½•
- [é…ç½®é€‰é¡¹è¯¦è§£](#é…ç½®é€‰é¡¹è¯¦è§£)
- [ä½¿ç”¨åœºæ™¯æŒ‡å—](#ä½¿ç”¨åœºæ™¯æŒ‡å—)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æˆæœ¬æ§åˆ¶](#æˆæœ¬æ§åˆ¶)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [ç¯å¢ƒå˜é‡é…ç½®](#ç¯å¢ƒå˜é‡é…ç½®)

## ğŸ›ï¸ é…ç½®é€‰é¡¹è¯¦è§£

### åŸºç¡€é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | å»ºè®®èŒƒå›´ |
|-----|------|-------|------|---------|
| `model` | string | `"x-ai/grok-4-fast:free"` | æ¨¡å‹åç§° | - |
| `temperature` | number | `0.3` | æ§åˆ¶å›ç­”çš„éšæœºæ€§ | 0.0-1.0 |
| `maxTokens` | number | `4096` | æœ€å¤§è¾“å‡ºé•¿åº¦ | 100-32768 |
| `topP` | number | `1.0` | æ ¸é‡‡æ ·æ¦‚ç‡ | 0.1-1.0 |
| `frequencyPenalty` | number | `0` | é¢‘ç‡æƒ©ç½š | -2.0-2.0 |
| `presencePenalty` | number | `0` | å­˜åœ¨æƒ©ç½š | -2.0-2.0 |
| `enableReasoning` | boolean | `false` | å¯ç”¨æ¨ç†æ¨¡å¼ | - |
| `streaming` | boolean | `true` | æµå¼å“åº” | - |
| `timeout` | number | `60000` | è¶…æ—¶æ—¶é—´(ms) | 10000-120000 |

### ğŸ¯ Temperature ä½¿ç”¨æŒ‡å—

```typescript
// ğŸ’¡ åˆ›æ„ä»»åŠ¡ (é«˜æ¸©åº¦)
const creativeLLM = getLLM({ temperature: 0.8 });
// é€‚ç”¨äºï¼šæ•…äº‹åˆ›ä½œã€è¥é”€æ–‡æ¡ˆã€å¤´è„‘é£æš´

// ğŸ¯ å¹³è¡¡ä»»åŠ¡ (ä¸­æ¸©åº¦) 
const balancedLLM = getLLM({ temperature: 0.3 });
// é€‚ç”¨äºï¼šä¸€èˆ¬å¯¹è¯ã€æ•™å­¦è§£é‡Šã€å†…å®¹æ€»ç»“

// ğŸ”’ ç²¾ç¡®ä»»åŠ¡ (ä½æ¸©åº¦)
const preciseLLM = getLLM({ temperature: 0.1 });
// é€‚ç”¨äºï¼šæ•°æ®åˆ†æã€ä»£ç ç”Ÿæˆã€äº‹å®æ€§å›ç­”
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯æŒ‡å—

### 1. ğŸ§  æ¨ç†å¯†é›†å‹ä»»åŠ¡

```typescript
const reasoningLLM = getReasoningLLM({
  maxTokens: 3000,
  temperature: 0.1,
});

// é€‚ç”¨åœºæ™¯ï¼š
// - å¤æ‚æ•°å­¦é—®é¢˜
// - é€»è¾‘æ¨ç†
// - å¤šæ­¥éª¤åˆ†æ
// - ç¼–ç¨‹é—®é¢˜è§£å†³
```

### 2. âœ¨ åˆ›æ„å†…å®¹ç”Ÿæˆ

```typescript
const creativeLLM = getCreativeLLM({
  temperature: 0.9,
  topP: 0.8,
  frequencyPenalty: 0.3,
  presencePenalty: 0.3,
});

// é€‚ç”¨åœºæ™¯ï¼š
// - è¯¾ç¨‹ä»‹ç»æ–‡æ¡ˆ
// - å­¦ä¹ æ¿€åŠ±å†…å®¹
// - äº’åŠ¨æ¸¸æˆè®¾è®¡
// - ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®
```

### 3. ğŸ“Š æ•°æ®åˆ†æä»»åŠ¡

```typescript
const analyticalLLM = getAnalyticalLLM({
  enableReasoning: true,
  temperature: 0.0,
  maxTokens: 2000,
});

// é€‚ç”¨åœºæ™¯ï¼š
// - å­¦ä¹ æ•°æ®åˆ†æ
// - ç”¨æˆ·è¡Œä¸ºæ´å¯Ÿ
// - è¯¾ç¨‹æ•ˆæœè¯„ä¼°
// - ä¸ªæ€§åŒ–æ¨è
```

### 4. ğŸ“š é•¿æ–‡æ¡£å¤„ç†

```typescript
const longContextLLM = getLongContextLLM({
  maxTokens: 32768,
  temperature: 0.2,
});

// é€‚ç”¨åœºæ™¯ï¼š
// - è¯¾ç¨‹å†…å®¹æ€»ç»“
// - æ–‡æ¡£é—®ç­”
// - çŸ¥è¯†æŠ½å–
// - è¯¾ç¨‹å¤§çº²ç”Ÿæˆ
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æµå¼å“åº”ä¼˜åŒ–

```typescript
// âœ… æ¨èï¼šå¯ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
const llm = getLLM({
  streaming: true,
  timeout: 30000, // è¾ƒçŸ­è¶…æ—¶æ—¶é—´
});

// å¤„ç†æµå¼å“åº”
async function handleStreaming(llm: ChatOpenAI, messages: any[]) {
  const stream = await llm.stream(messages);
  
  let buffer = '';
  for await (const chunk of stream) {
    buffer += chunk.content;
    // å®æ—¶æ›´æ–°UI
    updateUI(buffer);
  }
  
  return buffer;
}
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```typescript
// âœ… æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
async function batchProcess(queries: string[]) {
  const llm = getLLM({
    maxTokens: 1000, // è¾ƒå°tokené™åˆ¶
    temperature: 0.3,
  });

  const promises = queries.map(query => 
    llm.invoke([
      new SystemMessage("ç®€æ´å›ç­”"),
      new HumanMessage(query)
    ])
  );

  return await Promise.all(promises);
}
```

### 3. ç¼“å­˜ç­–ç•¥

```typescript
// âœ… å®ç°æ™ºèƒ½ç¼“å­˜
const responseCache = new Map<string, any>();

async function getCachedResponse(query: string, llm: ChatOpenAI) {
  const cacheKey = hashQuery(query);
  
  if (responseCache.has(cacheKey)) {
    console.log('ğŸ¯ Cache hit');
    return responseCache.get(cacheKey);
  }

  const response = await llm.invoke([
    new HumanMessage(query)
  ]);

  responseCache.set(cacheKey, response);
  return response;
}
```

## ğŸ’° æˆæœ¬æ§åˆ¶

### 1. æ™ºèƒ½é…ç½®é€‰æ‹©

```typescript
function getOptimalConfig(taskType: string, contentLength: number) {
  if (taskType === 'simple' && contentLength < 100) {
    return {
      enableReasoning: false,
      maxTokens: 300,
      temperature: 0.3,
    };
  }
  
  if (taskType === 'complex' || contentLength > 1000) {
    return {
      enableReasoning: true,
      maxTokens: 2000,
      temperature: 0.1,
    };
  }

  return DEFAULT_GROK_CONFIG;
}
```

### 2. è¯·æ±‚å»é‡

```typescript
const requestDeduplicator = new Map<string, Promise<any>>();

async function deduplicatedRequest(query: string, llm: ChatOpenAI) {
  const key = hashQuery(query);
  
  if (requestDeduplicator.has(key)) {
    console.log('ğŸ”„ Request deduplication');
    return requestDeduplicator.get(key);
  }

  const promise = llm.invoke([new HumanMessage(query)]);
  requestDeduplicator.set(key, promise);
  
  // æ¸…ç†è¿‡æœŸè¯·æ±‚
  setTimeout(() => requestDeduplicator.delete(key), 60000);
  
  return promise;
}
```

### 3. ç”¨é‡ç›‘æ§

```typescript
class UsageTracker {
  private requests = 0;
  private tokens = 0;
  private costs = 0;

  track(tokens: number, cost: number) {
    this.requests++;
    this.tokens += tokens;
    this.costs += cost;
    
    console.log(`ğŸ“Š Usage: ${this.requests} requests, ${this.tokens} tokens, $${this.costs.toFixed(4)}`);
  }

  getDailyReport() {
    return {
      requests: this.requests,
      tokens: this.tokens,
      estimatedCost: this.costs,
      avgTokensPerRequest: this.tokens / this.requests,
    };
  }
}
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†

### 1. é‡è¯•æœºåˆ¶

```typescript
async function resilientLLMCall(
  llm: ChatOpenAI, 
  messages: any[], 
  maxRetries = 3
) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await llm.invoke(messages);
    } catch (error) {
      console.warn(`âŒ Attempt ${attempt} failed:`, error);
      
      if (attempt === maxRetries) {
        throw new Error(`æ‰€æœ‰é‡è¯•å¤±è´¥: ${error}`);
      }
      
      // æŒ‡æ•°é€€é¿
      await new Promise(resolve => 
        setTimeout(resolve, Math.pow(2, attempt) * 1000)
      );
    }
  }
}
```

### 2. é™çº§ç­–ç•¥

```typescript
async function fallbackLLMCall(primaryQuery: string, fallbackQuery?: string) {
  try {
    // å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å¼
    const reasoningLLM = getReasoningLLM();
    return await reasoningLLM.invoke([new HumanMessage(primaryQuery)]);
  } catch (error) {
    console.warn('ğŸ”„ Falling back to basic model');
    
    try {
      // é™çº§åˆ°åŸºç¡€æ¨¡å‹
      const basicLLM = getLLM({ enableReasoning: false });
      const query = fallbackQuery || primaryQuery;
      return await basicLLM.invoke([new HumanMessage(query)]);
    } catch (fallbackError) {
      // æœ€ç»ˆé™çº§åˆ°é™æ€å›ç­”
      return {
        content: 'æŠ±æ­‰ï¼ŒAIåŠ©æ‰‹æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚'
      };
    }
  }
}
```

### 3. è¾“å…¥éªŒè¯

```typescript
function validateInput(input: string): { valid: boolean; error?: string } {
  if (!input || input.trim().length === 0) {
    return { valid: false, error: 'è¾“å…¥ä¸èƒ½ä¸ºç©º' };
  }
  
  if (input.length > 10000) {
    return { valid: false, error: 'è¾“å…¥è¿‡é•¿ï¼Œè¯·åˆ†æ®µå¤„ç†' };
  }
  
  // æ£€æŸ¥æ•æ„Ÿå†…å®¹
  const sensitivePattern = /password|token|secret|key/i;
  if (sensitivePattern.test(input)) {
    return { valid: false, error: 'è¾“å…¥åŒ…å«æ•æ„Ÿä¿¡æ¯' };
  }
  
  return { valid: true };
}
```

## ğŸ”§ ç¯å¢ƒå˜é‡é…ç½®

### å¿…éœ€ç¯å¢ƒå˜é‡

```bash
# OpenRouter APIå¯†é’¥ï¼ˆå¿…éœ€ï¼‰
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxx

# ç«™ç‚¹ä¿¡æ¯ï¼ˆç”¨äºOpenRouteræ’åï¼‰
NEXT_PUBLIC_SITE_URL=https://studify.app
NEXT_PUBLIC_SITE_NAME=Studify

# å¯é€‰é…ç½®
GROK_DEFAULT_MODEL=x-ai/grok-4-fast:free
GROK_MAX_TOKENS=4096
GROK_TEMPERATURE=0.3
```

### å¼€å‘ç¯å¢ƒé…ç½®

```typescript
// lib/langChain/config.ts
export const getEnvironmentConfig = () => ({
  apiKey: process.env.OPENROUTER_API_KEY!,
  siteUrl: process.env.NEXT_PUBLIC_SITE_URL || 'http://localhost:3000',
  siteName: process.env.NEXT_PUBLIC_SITE_NAME || 'Studify Development',
  defaultModel: process.env.GROK_DEFAULT_MODEL || 'x-ai/grok-4-fast:free',
  maxTokens: parseInt(process.env.GROK_MAX_TOKENS || '4096'),
  temperature: parseFloat(process.env.GROK_TEMPERATURE || '0.3'),
});
```

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### 1. æ€§èƒ½ç›‘æ§

```typescript
class PerformanceMonitor {
  async trackLLMCall<T>(
    operation: string,
    fn: () => Promise<T>
  ): Promise<T> {
    const startTime = Date.now();
    
    try {
      const result = await fn();
      const duration = Date.now() - startTime;
      
      console.log(`âœ… ${operation} completed in ${duration}ms`);
      
      // å‘é€åˆ°åˆ†ææœåŠ¡
      this.sendMetrics({
        operation,
        duration,
        status: 'success',
        timestamp: new Date().toISOString(),
      });
      
      return result;
    } catch (error) {
      const duration = Date.now() - startTime;
      
      console.error(`âŒ ${operation} failed after ${duration}ms:`, error);
      
      this.sendMetrics({
        operation,
        duration,
        status: 'error',
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString(),
      });
      
      throw error;
    }
  }

  private sendMetrics(metrics: any) {
    // å‘é€åˆ°ä½ çš„åˆ†ææœåŠ¡ï¼ˆå¦‚Mixpanelã€Google Analyticsç­‰ï¼‰
    // analytics.track('llm_call', metrics);
  }
}
```

### 2. ç»“æ„åŒ–æ—¥å¿—

```typescript
import { createLogger } from './logger';

const logger = createLogger('LangChain');

export async function loggedLLMCall(
  llm: ChatOpenAI,
  messages: any[],
  context?: any
) {
  const requestId = generateRequestId();
  
  logger.info('ğŸš€ Starting LLM call', {
    requestId,
    messageCount: messages.length,
    context,
  });

  try {
    const response = await llm.invoke(messages);
    
    logger.info('âœ… LLM call completed', {
      requestId,
      responseLength: response.content.length,
    });
    
    return response;
  } catch (error) {
    logger.error('âŒ LLM call failed', {
      requestId,
      error: error instanceof Error ? error.message : 'Unknown error',
    });
    
    throw error;
  }
}
```

## ğŸ¯ Studifyç‰¹å®šç”¨ä¾‹

### 1. ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®

```typescript
export async function generatePersonalizedRecommendation(
  userProfile: any,
  learningHistory: any,
  currentCourse: any
) {
  const analyticalLLM = getAnalyticalLLM({
    maxTokens: 2000,
    enableReasoning: true,
  });

  const prompt = `
åŸºäºä»¥ä¸‹ä¿¡æ¯ä¸ºå­¦ç”Ÿç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®ï¼š

ç”¨æˆ·èµ„æ–™ï¼š${JSON.stringify(userProfile)}
å­¦ä¹ å†å²ï¼š${JSON.stringify(learningHistory)}
å½“å‰è¯¾ç¨‹ï¼š${JSON.stringify(currentCourse)}

è¯·æä¾›ï¼š
1. å­¦ä¹ è¿›åº¦åˆ†æ
2. ä¸ªæ€§åŒ–å»ºè®®
3. å­¦ä¹ è·¯å¾„ä¼˜åŒ–
4. æ½œåœ¨å›°éš¾ç‚¹é¢„è­¦
  `;

  return await analyticalLLM.invoke([
    new SystemMessage("ä½ æ˜¯Studifyçš„AIå­¦ä¹ é¡¾é—®ã€‚"),
    new HumanMessage(prompt),
  ]);
}
```

### 2. æ™ºèƒ½è¯¾ç¨‹å†…å®¹ç”Ÿæˆ

```typescript
export async function generateCourseContent(
  topic: string,
  level: 'beginner' | 'intermediate' | 'advanced',
  duration: number
) {
  const creativeLLM = getCreativeLLM({
    temperature: 0.7,
    maxTokens: 3000,
  });

  return await creativeLLM.invoke([
    new SystemMessage(`ä½ æ˜¯èµ„æ·±æ•™è‚²ä¸“å®¶ï¼Œä¸“é—¨è®¾è®¡${level}çº§åˆ«çš„è¯¾ç¨‹å†…å®¹ã€‚`),
    new HumanMessage(`
ä¸º"${topic}"ä¸»é¢˜è®¾è®¡${duration}åˆ†é’Ÿçš„è¯¾ç¨‹å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. å­¦ä¹ ç›®æ ‡
2. è¯¾ç¨‹å¤§çº²
3. é‡ç‚¹çŸ¥è¯†ç‚¹
4. å®è·µç»ƒä¹ 
5. è¯„ä¼°æ–¹å¼
    `),
  ]);
}
```

è¿™äº›é…ç½®å’Œæœ€ä½³å®è·µå°†å¸®åŠ©ä½ åœ¨Studifyå¹³å°ä¸­å……åˆ†å‘æŒ¥Grok-4çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„æ€§èƒ½å’Œæˆæœ¬æ§åˆ¶ã€‚
