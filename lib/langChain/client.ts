// lib/langChain/client.ts - Optimized OpenRouter + Grok-4 Configuration with Multi-Key Support
import { ChatOpenAI } from "@langchain/openai";
import { apiKeyManager } from "./api-key-manager";

// Grok-4 æ¨¡å‹é…ç½®é€‰é¡¹
export interface GrokConfig {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  enableReasoning?: boolean;
  streaming?: boolean;
  timeout?: number;
  keySelectionStrategy?: 'round_robin' | 'least_used' | 'best_performance';
  maxRetries?: number;
}

// é»˜è®¤é…ç½®
const DEFAULT_GROK_CONFIG: GrokConfig = {
  model: "deepseek/deepseek-chat-v3.1:free",
  temperature: 0.3,
  maxTokens: 15000, // DeepSeekæ”¯æŒæœ€å¤§64K tokensï¼Œä½†å®é™…ä½¿ç”¨å»ºè®®4K-8K
  topP: 1.0,
  frequencyPenalty: 0,
  presencePenalty: 0,
  enableReasoning: false, // é»˜è®¤å…³é—­æ¨ç†æ¨¡å¼ä»¥èŠ‚çœæˆæœ¬
  streaming: true, // å¯ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
  timeout: 60000, // 60ç§’è¶…æ—¶
  keySelectionStrategy: 'round_robin', // é»˜è®¤è½®è¯¢ç­–ç•¥
  maxRetries: 3, // é»˜è®¤é‡è¯•3æ¬¡
};

// è·å–ç«™ç‚¹ä¿¡æ¯ç”¨äºOpenRouteræ’å
const getSiteInfo = () => {
  const isDevelopment = process.env.NODE_ENV === 'development';
  const defaultUrl = isDevelopment 
    ? "http://localhost:3000" 
    : "https://studify-platform.vercel.app";
    
  // åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œä¼˜å…ˆä½¿ç”¨ localhost
  const siteUrl = isDevelopment 
    ? "http://localhost:3000"
    : (process.env.NEXT_PUBLIC_SITE_URL || defaultUrl);
    
  return {
    siteUrl,
    siteName: process.env.NEXT_PUBLIC_SITE_NAME || "Studify",
  };
};

/**
 * åˆ›å»ºä¼˜åŒ–çš„Grok-4 LLMå®ä¾‹ - æ”¯æŒå¤škeyè½®æ¢
 * @param config è‡ªå®šä¹‰é…ç½®é€‰é¡¹
 * @returns ChatOpenAIå®ä¾‹
 */
export async function getLLM(config: Partial<GrokConfig> = {}) {
  // Set dummy OPENAI_API_KEY to prevent LangChain internal checks
  // This is safe because we override the baseURL to OpenRouter


  const finalConfig = { ...DEFAULT_GROK_CONFIG, ...config };
  const { siteUrl, siteName } = getSiteInfo();

  // ä»key managerè·å–å¯ç”¨çš„API key
  const { key: apiKey, name: keyName } = await apiKeyManager.getAvailableKey(
    finalConfig.keySelectionStrategy
  );

  console.log(`ğŸ”‘ Using OpenRouter key: ${keyName} (${apiKey.substring(0, 12)}...${apiKey.substring(apiKey.length - 4)})`);

  // æ„é€ æ¨¡å‹å‚æ•°
  const modelParams: any = {
    model: finalConfig.model,
    temperature: finalConfig.temperature,
    maxTokens: finalConfig.maxTokens,
    topP: finalConfig.topP,
    frequencyPenalty: finalConfig.frequencyPenalty,
    presencePenalty: finalConfig.presencePenalty,
    streaming: finalConfig.streaming,
    timeout: finalConfig.timeout,
    maxRetries: finalConfig.maxRetries,
  };

  // Grok-4æ¨ç†æ¨¡å¼é…ç½®
  if (finalConfig.enableReasoning) {
    modelParams.reasoning = { enabled: true };
  }

  const chatOpenAI = new ChatOpenAI({
    ...modelParams,
    openAIApiKey: apiKey, // LangChain still needs this parameter
    configuration: {
      baseURL: "https://openrouter.ai/api/v1",
      defaultHeaders: {
        "Authorization": `Bearer ${apiKey}`, // OpenRouter éœ€è¦è¿™ä¸ªå¤´
        "HTTP-Referer": siteUrl, // ç”¨äºOpenRouteræ’åç»Ÿè®¡
        "X-Title": siteName, // ç”¨äºOpenRouteræ’åæ˜¾ç¤º
      },
    },
    callbacks: [{
      handleLLMEnd: async () => {
        await apiKeyManager.recordUsage(keyName, true);
      },
      handleLLMError: async (error) => {
        await apiKeyManager.recordUsage(keyName, false, error);
      }
    }]
  });

  // æ·»åŠ keyä¿¡æ¯åˆ°å®ä¾‹
  (chatOpenAI as any).__keyName = keyName;
  return chatOpenAI;
}

/**
 * è·å–æ¨ç†æ¨¡å¼çš„DeepSeekå®ä¾‹ï¼ˆæ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä½†æˆæœ¬æ›´é«˜ï¼‰
 */
export function getReasoningLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    enableReasoning: true,
    temperature: 0.1, // æ¨ç†æ¨¡å¼ä½¿ç”¨æ›´ä½çš„æ¸©åº¦
    model: "openai/gpt-4o", // Use GPT-4o for reasoning tasks
  });
}

/**
 * è·å–åˆ›æ„å†™ä½œä¸“ç”¨çš„DeepSeekå®ä¾‹
 */
export function getCreativeLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: "openai/gpt-4o-mini", // Use GPT-4o-mini for creative tasks
    temperature: 0.8, // é«˜æ¸©åº¦å¢åŠ åˆ›æ„æ€§
    topP: 0.9,
    frequencyPenalty: 0.1,
    presencePenalty: 0.1,
  });
}

/**
 * è·å–åˆ†æä¸“ç”¨çš„DeepSeekå®ä¾‹
 */
export function getAnalyticalLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: "openai/gpt-4o", // Use GPT-4o for analytical tasks
    temperature: 0.1, // ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
    topP: 0.95,
    enableReasoning: true, // å¯ç”¨æ¨ç†æå‡åˆ†æè´¨é‡
  });
}

/**
 * è·å–å¤§ä¸Šä¸‹æ–‡çª—å£çš„DeepSeekå®ä¾‹ï¼ˆç”¨äºé•¿æ–‡æ¡£å¤„ç†ï¼‰
 */
export function getLongContextLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: "openai/gpt-4o", // Use GPT-4o for long context tasks
    maxTokens: 32768, // ä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£
    temperature: 0.2,
  });
}

/**
 * è·å–è§†è§‰æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºå›¾æ–‡ç†è§£ï¼‰- ä½¿ç”¨Kimi VLæ¨¡å‹
 */
export function getVisionLLM(config: Partial<GrokConfig> = {}) {
  return getLLM({
    ...config,
    model: "openai/gpt-4o", // Use GPT-4o for vision tasks (supports images and function calling)
    temperature: 0.3,
    maxTokens: 4096,
  });
}

// å¯¼å‡ºé…ç½®å¸¸é‡ä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
export { DEFAULT_GROK_CONFIG };
